[{"authors":["admin"],"categories":null,"content":"I am a Ph.D. candidate at the Robotics and Computer Vision Lab at KAIST, South Korea, under the supervision of Prof. Kweon In So. My research focuses on deep learning with a focus on robustness and security. Prior to the PhD course, I worked at LS Electric for 2 and half years. I got two two master degrees: Engineering \u0026amp; policy Analysis at TU Delft and Electrical Engineering at Harbin Institute of Technology.\n","date":1601596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1601596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://chaoningzhang.github.io/author/chaoning-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chaoning-zhang/","section":"authors","summary":"I am a Ph.D. candidate at the Robotics and Computer Vision Lab at KAIST, South Korea, under the supervision of Prof. Kweon In So. My research focuses on deep learning with a focus on robustness and security.","tags":null,"title":"Chaoning Zhang","type":"authors"},{"authors":[],"categories":null,"content":"","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"5df7b664ee5f1fd972fae603fa809b5e","permalink":"https://chaoningzhang.github.io/talk/2d3d.ai/","publishdate":"2020-08-08T00:00:00Z","relpermalink":"/talk/2d3d.ai/","section":"talk","summary":"Coming soon","tags":[],"title":"Adversarial Machine Learning and Beyond","type":"talk"},{"authors":["Chaoning Zhang","Chaoning Zhang","In So Kweon"],"categories":null,"content":"","date":1601596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601596800,"objectID":"3896c8ba6539e8b8e7e545863456e4b9","permalink":"https://chaoningzhang.github.io/publication/batch_norm_increases_adversarial_vulnerability/","publishdate":"2020-10-02T00:00:00Z","relpermalink":"/publication/batch_norm_increases_adversarial_vulnerability/","section":"publication","summary":"Batch normalization (BN) has been widely used in modern deep neural networks (DNNs) due to fast convergence. BN is observed to increase the model accuracy while at the cost of adversarial robustness. We conjecture that the increased adversarial vulnerability is caused by BN shifting the model to rely more on non-robust features (NRFs). Our exploration finds that other normalization techniques also increase adversarial vulnerability and our conjecture is also supported by analyzing the model corruption robustness and feature transferability. With a classifier DNN defined as a feature set F we propose a framework for disentangling F robust usefulness into F usefulness and F robustness. We adopt a local linearity based metric, termed LIGS, to define and quantify F robustness. Measuring the F robustness with the LIGS provides direct insight on the feature robustness shift independent of usefulness. Moreover, the LIGS trend during the whole training stage sheds light on the order of learned features, i.e. from RFs (robust features) to NRFs, or vice versa. Our work analyzes how BN and other factors influence the DNN from the feature perspective. Prior works mainly adopt accuracy to evaluate their influence regarding F usefulness, while we believe evaluating F robustness is equally important, for which our work fills the gap.","tags":["Batch Normalization","Adversarial Machine Learning"],"title":"Batch Normalization Increases Adversarial Vulnerability: Disentangling Usefulness and Robustness of Model Features","type":"publication"},{"authors":["Chaoning Zhang","Chaoning Zhang","Adil Karjauv","In So Kweon"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"ac037e1428db843321ea7bed9fc00e7a","permalink":"https://chaoningzhang.github.io/publication/revisiting_batch_normalization/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/revisiting_batch_normalization/","section":"publication","summary":"Modern deep neural networks (DNN) have demonstrated remarkable success in image recognition tasks when the test dataset and training dataset are from the same distribution. In practical applications, however, this assumption is often not valid and results in performance drop when there is a domain shift. For example, the performance of DNNs trained on clean images has been shown to decrease when the test images have common corruptions, limiting their use in performance-sensitive applications. In this work, we interpret corruption robustness as a domain shift problem and propose to rectify batch normalization (BN) statistics for improving model robustness. This shift from the clean domain to the corruption domain can be interpreted as a style shift that is represented by the BN statistics. Straightforwardly, adapting BN statistics is beneficial for rectifying this style shift. Specifically, we find that simply estimating and adapting the BN statistics on a few (32 for instance) representation samples, without retraining the model, improves the corruption robustness by a large margin on several benchmark datasets with a wide range of model architectures. For example, on ImageNet-C, statistics adaptation improves the top1 accuracy from 40.2% to 49%. Moreover, we find that this technique can further improve state-of-the-art robust models from 59.0% to 63.5%.","tags":["Batch Normalization","Adversarial Machine Learning"],"title":"Revisiting Batch Normalization for Improving Corruption Robustness","type":"publication"},{"authors":["Chaoning Zhang","Chaoning Zhang","Tooba Imtiaz","In So Kweon"],"categories":null,"content":"","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"de4328e8b155c97f78bd75415f1a75a0","permalink":"https://chaoningzhang.github.io/publication/double_targeted_attack/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/publication/double_targeted_attack/","section":"publication","summary":"Despite their impressive performance, deep neural networks (DNNs) are widely known to be vulnerable to adversarial attacks, which makes it challenging for them to be deployed in security-sensitive applications, such as autonomous driving. Image-dependent perturbations can fool a network for one specific image, while universal adversarial perturbations are capable of fooling a network for samples from all classes without selection. We introduce a double targeted universal adversarial perturbations (DT-UAPs) to bridge the gap between the instance-discriminative image-dependent perturbations and the generic universal perturbations. This universal perturbation attacks one targeted source class to sink class, while having a limited adversarial effect on other non-targeted source classes, for avoiding raising suspicions. Targeting the source and sink class simultaneously, we term it double targeted attack (DTA). This provides an attacker with the freedom to perform precise attacks on a DNN model while raising little suspicion. We show the effectiveness of the proposed DTA algorithm on a wide range of datasets and also demonstrate its potential as a physical attack.","tags":["Universal Adversarial Perturbations","Adversarial Machine Learning"],"title":"Double Targeted Universal Adversarial Perturbations","type":"publication"},{"authors":["Chaoning Zhang","Chaoning Zhang","Tooba Imtiaz","In So Kweon"],"categories":null,"content":"","date":1591228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591228800,"objectID":"24ae1f88c3045ffe3eb5a89570e98351","permalink":"https://chaoningzhang.github.io/publication/understanding_uaps/","publishdate":"2020-06-04T00:00:00Z","relpermalink":"/publication/understanding_uaps/","section":"publication","summary":"A wide variety of works have explored the reason for the existence of adversarial examples, but there is no consensus on the explanation. We propose to treat the DNN logits as a vector for feature representation, and exploit them to analyze the mutual influence of two independent inputs based on the Pearson correlation coefficient (PCC). We utilize this vector representation to understand adversarial examples by disentangling the clean images and adversarial perturbations, and analyze their influence on each other. Our results suggest a new perspective towards the relationship between images and universal perturbations: Universal perturbations contain dominant features, and images behave like noise to them. This feature perspective leads to a new method for generating targeted universal adversarial perturbations using random source images. We are the first to achieve the challenging task of a targeted universal attack without utilizing original training data. Our approach using a proxy dataset achieves comparable performance to the state-of-the-art baselines which utilize the original training dataset.","tags":["Universal Adversarial Perturbations","Adversarial Machine Learning"],"title":"Understanding Adversarial Examples from the Mutual Influence of Images and Perturbations","type":"publication"},{"authors":["Chaoning Zhang","Chaoning Zhang","Tooba Imtiaz","In So Kweon"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"d1bf35bbc57d64d75e5497724d3d345f","permalink":"https://chaoningzhang.github.io/publication/data_from_model/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/data_from_model/","section":"publication","summary":"The essence of deep learning is to exploit data to train a deep neural network (DNN) model. This work explores the reverse process of generating data from a model, attempting to reveal the relationship between the data and the model. We repeat the process of Data to Model (DtM) and Data from Model (DfM) in sequence and explore the loss of feature mapping information by measuring the accuracy drop on the original validation dataset. We perform this experiment for both a non-robust and robust origin model. Our results show that the accuracy drop is limited even after multiple sequences of DtM and DfM, especially for robust models. The success of this cycling transformation can be attributed to the shared feature mapping existing in data and model. Using the same data, we observe that different DtM processes result in models having different features, especially for different network architecture families, even though they achieve comparable performance.","tags":["Data from Model","Adversarial Machine Learning"],"title":"Data from Model: Extracting Data from Non-robust and Robust Models","type":"publication"},{"authors":["Chaoning Zhang","Chaoning Zhang","Tooba Imtiaz","In So Kweon"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"d6db0be977490032b63d9a70680200f2","permalink":"https://chaoningzhang.github.io/publication/cd_uap/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/publication/cd_uap/","section":"publication","summary":"A single universal adversarial perturbation (UAP) can be added to all natural images to change most of their predicted class labels. It is of high practical relevance for an attacker to have flexible control over the targeted classes to be attacked, however, the existing UAP method attacks samples from all classes. In this work, we propose a new universal attack method to generate a single perturbation that fools a target network to misclassify only a chosen group of classes, while having limited influence on the remaining classes. Since the proposed attack generates a universal adversarial perturbation that is discriminative to targeted and non-targeted classes, we term it class discriminative universal adversarial perturbation (CD-UAP). We propose one simple yet effective algorithm framework, under which we design and compare various loss function configurations tailored for the class discriminative universal attack. The proposed approach has been evaluated with extensive experiments on various benchmark datasets. Additionally, our proposed approach achieves state-of-the-art performance for the original task of UAP attacking all classes, which demonstrates the effectiveness of our approach.","tags":["Universal Adversarial Examples","Adversarial Machine Learning"],"title":"CD-UAP: Class Discriminative Universal Adversarial Perturbation","type":"publication"},{"authors":["Ho-Deok Jang","Sanghyun Woo","Chaoning Zhang","Jinsun Park","In So Kweon"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"faeeec87da39aca065ab2112eefcba94","permalink":"https://chaoningzhang.github.io/publication/propose_and_attend_ssd/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/propose_and_attend_ssd/","section":"publication","summary":"We present a simple yet effective prediction module for a one-stage detector. The main process is conducted in a coarse-to-fine manner. First, the module roughly adjusts the default boxes to well capture the extent of target objects in an image. Second, given the adjusted boxes, the module aligns the receptive field of the convolution filters accordingly, not requiring any embedding layers. Both steps build a propose-and-attend mechanism, mimicking two-stage detectors in a highly efficient manner. To verify its effectiveness, we apply the proposed module to a basic one-stage detector SSD. Our final model achieves an accuracy comparable to that of state-of-the-art detectors while using a fraction of their model parameters and computational overheads. Moreover, we found that the proposed module has two strong applications. 1) The module can be successfully integrated into a lightweight backbone, further pushing the efficiency of the one-stage detector. 2) The module also allows train-from-scratch without relying on any sophisticated base networks as previous methods do.","tags":["Object Detection","Network Design"],"title":"Propose-and-Attend Single Shot Detector","type":"publication"},{"authors":["Chaoning Zhang","Francois Rameau","Seokju Lee","Junsik Kim","Chaoning Zhang","Dawit Mureja Argaw","Jean-Charles Bazin","In So Kweon"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"aac0bdf7e43eef415e4820ff36483f74","permalink":"https://chaoningzhang.github.io/publication/revisiting_residual_networks/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/revisiting_residual_networks/","section":"publication","summary":"Residual networks (ResNets) with an identity shortcut have been widely used in various computer vision tasks due to their compelling performance and simple design. In this paper we revisit ResNet identity shortcut and propose RGSNets which are based on a new nonlinear ReLU Group Normalization (RG) shortcut, outperforming the existing ResNet by a relatively large margin. Our work is inspired by previous findings that there is a trade-off between representational power and gradient stability in deep networks and that the identity shortcut reduces the representational power. Our proposed nonlinear RG shortcut can contribute to effectively utilizing the representational power of relatively shallow networks and outperform much (3 or 4 times) deeper ResNets, which demonstrates the high efficiency of RG shortcut. Moreover, we have explored variations of RGSNets, and our experimental result shows that Res-RGSNet combining the proposed RG shortcut with the existing identity shortcut achieves the best performance and is robust to network depth. Our code and models are publicly available on our website.","tags":["Network Design","Deep Learning"],"title":"Revisiting Residual Networks with Nonlinear Shortcuts","type":"publication"},{"authors":["Moonyoung Lee","Yujin Heo","Jinyong Park","Hyundae Yang","Ho-Deok Jang","Chaoning Zhang","Hyunsub Park","In So Kweon","Jun-Ho Oh"],"categories":null,"content":"","date":1564012800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564012800,"objectID":"15dccb463b5bb6f59b7ca4411235f6ce","permalink":"https://chaoningzhang.github.io/publication/m_hubo_robotic_butler/","publishdate":"2019-07-25T00:00:00Z","relpermalink":"/publication/m_hubo_robotic_butler/","section":"publication","summary":"As the aging population grows at a rapid rate, there is an ever growing need for service robot platforms that can provide daily assistance at practical speed with reliable performance. In order to assist with daily tasks such as fetching a beverage, a service robot must be able to perceive its environment and generate corresponding motion trajectories. This becomes a challenging and computationally complex problem when the environment is unknown and thus the path planner must sample numerous trajectories that often are sub-optimal, extending the execution time. To address this issue, we propose a unique strategy of integrating a 3D object detection pipeline with a kinematically optimal manipulation planner to significantly increase speed performance at runtime. In addition, we develop a new robotic butler system for a wheeled humanoid that is capable of fetching requested objects at 24% of the speed a human needs to fulfill the same task. The proposed system was evaluated and demonstrated in a real-world environment setup as well as in public exhibition.","tags":["Mobile Robot Navigation","Robot Vision"],"title":"Fast Perception, Planning, and Execution for a Robotic Butler: Wheeled Humanoid M-Hubo","type":"publication"},{"authors":["Seung-Ho Han","Ho-Jin Choi","Chaoning Zhang","Jorge Loaiciga"],"categories":null,"content":"","date":1515974400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515974400,"objectID":"dfee98c43434a48d75c4d6277d286595","permalink":"https://chaoningzhang.github.io/publication/sensor_based_mobile_robot_navigation/","publishdate":"2018-01-15T00:00:00Z","relpermalink":"/publication/sensor_based_mobile_robot_navigation/","section":"publication","summary":"Navigation tasks for mobile robots have been widely studied over past several years. More recently, there have been many attempts to introduce the usage of machine learning algorithms. Deep learning techniques are of special importance because they have achieved excellent performance in various fields, including robot navigation. Deep learning methods, however, require considerable amount of data for training deep learning models and their results may be difficult to interpret for researchers. To address this issue, we propose a novel model for mobile robot navigation using deep reinforcement learning. In our navigation tasks, no information about the environment is given to the robot beforehand. Additionally, the positions of obstacles and goal change in every episode. In order to succeed under these conditions, we combine several Q-learning techniques that are considered to be state-of-the-art. We first provide a description of our model and then verify it through a series of experiments.","tags":["Mobile Robot Navigation","Reinforcement Learning"],"title":"Sensor-Based Mobile Robot Navigation via Deep Reinforcement Learning","type":"publication"}]